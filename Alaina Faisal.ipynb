{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/mini-project-2-Abo/blob/main/Alaina%20Faisal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1YPB14iE2XOl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import pickle\n",
        "import csv\n",
        "\n",
        "import tensorflow as tf\n",
        "from nltk.corpus import stopwords\n",
        "import keras.backend as K\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense,Input,Embedding,Dropout,Conv1D,MaxPooling1D,GlobalMaxPooling1D,Dropout,Bidirectional,Flatten,BatchNormalization,SimpleRNN,LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "#import transformers\n",
        "#import tokenizers\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import pipeline\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "#plotting\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqPR5V5imAqg",
        "outputId": "d9ae6ae2-8271-4e1b-e70b-7aeb39f74d00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "The dataset being used is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the Twitter API. The tweets have been annotated (0 = Negative, 4 = Positive) and they can be used to detect sentiment.\n",
        "\n",
        "It contains the following 6 fields:\n",
        "\n",
        "*  sentiment: the polarity of the tweet (0 = negative, 4 = positive)\n",
        "*  ids: The id of the tweet (2087)\n",
        "*  date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "*  flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "*  user: the user that tweeted (robotickilldozr)\n",
        "*  text: the text of the tweet (Lyx is cool)\n",
        "We require only the sentiment and text fields, so we discard the rest.\n",
        "\n",
        "Furthermore, we're changing the sentiment field so that it has new values to reflect the sentiment. (0 = Negative, 1 = Positive)"
      ],
      "metadata": {
        "id": "1i4c9O5I5dNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data could be downloaded and imported from https://www.kaggle.com/datasets/ferno2/training1600000processednoemoticoncsv"
      ],
      "metadata": {
        "id": "BQyxYAIm9f8A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "dAQ2etDAIZi_",
        "outputId": "96d9c755-2e89-48eb-f692-fe94f928f35b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "id": "ys5TF9MUJ_bR",
        "outputId": "e9fd8cca-98d5-4eb8-e45e-1c45247c7677",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "metadata": {
        "id": "yYKEBK-jJAVz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/simulate111/mini-project-2-Abo/main/kaggle.json\"\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Parse the JSON content\n",
        "    kaggle_config = response.json()\n",
        "\n",
        "    # Save the Kaggle API key to a file\n",
        "    with open('/content/kaggle.json', 'w') as file:\n",
        "        json.dump(kaggle_config, file)\n",
        "\n",
        "    # Set the Kaggle API key environment variable\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = '/content/'\n",
        "\n",
        "    # Download the dataset\n",
        "    !kaggle datasets download -d ferno2/training1600000processednoemoticoncsv\n",
        "\n",
        "    # Unzip the downloaded file\n",
        "    !unzip -q training1600000processednoemoticoncsv.zip -d dataset\n",
        "\n",
        "    # Check the contents of the dataset folder\n",
        "    !ls dataset\n",
        "else:\n",
        "    print(f\"Failed to fetch Kaggle API key. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "lSLy1lswOs2K",
        "outputId": "a6753a3a-af5f-4c16-e4c8-8e3bb307f278",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "training1600000processednoemoticoncsv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace dataset/training.1600000.processed.noemoticon.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: training.1600000.processed.noemoticon.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip training1600000processednoemoticoncsv.zip\n"
      ],
      "metadata": {
        "id": "wkKz_JGSJZYT",
        "outputId": "c9ecb3a0-f8d8-47ac-9352-91f29318ddad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  training1600000processednoemoticoncsv.zip\n",
            "replace training.1600000.processed.noemoticon.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/training.1600000.processed.noemoticon.csv',encoding='latin',header=None)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hB7QqiaJ4R94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns=['sentiment', 'id', 'date', 'query', 'user_id', 'text']"
      ],
      "metadata": {
        "id": "qMyLMn06OUT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "eShoSil_OYRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('length of data is', len(df))\n"
      ],
      "metadata": {
        "id": "1yX_2p7fOiKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "H94j5ZE5O_DS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame containing the Sentiment140 dataset\n",
        "\n",
        "# Convert 'date' column to datetime format\n",
        "df['date'] = pd.to_datetime(df['date'], format='%a %b %d %H:%M:%S PDT %Y')\n",
        "\n",
        "# Extract just the date part (optional, if you want to aggregate by day regardless of time)\n",
        "df['just_date'] = df['date'].dt.date\n",
        "\n",
        "# Map numerical sentiment to categorical for better readability\n",
        "df['sentiment_category'] = df['sentiment'].map({0: 'Negative', 4: 'Positive'})\n",
        "\n",
        "# Group by the new 'just_date' column and sentiment category, then count occurrences\n",
        "daily_sentiments = df.groupby(['just_date', 'sentiment_category']).size().unstack().fillna(0)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "daily_sentiments.plot(kind='line')\n",
        "plt.title('Daily Sentiment Trends')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Tweets')\n",
        "plt.legend(['Negative', 'Positive'])\n",
        "plt.xticks(rotation=45)  # Rotate date labels for better readability\n",
        "plt.tight_layout()  # Adjust layout to not cut off labels\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Xio-FHZgDqQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "JpljOpZVO43E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Count = df.sentiment.value_counts()\n",
        "Count"
      ],
      "metadata": {
        "id": "Eh73unb0PC2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'] = df['sentiment'].replace(4,1)\n",
        "df=df[['text','sentiment']]\n",
        "df.sentiment.value_counts()"
      ],
      "metadata": {
        "id": "_zB5MtglPjxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('white')\n",
        "plot = df.groupby('sentiment').count().plot(kind='bar', title='Data',\n",
        "                                          legend=False, color=['skyblue', 'salmon'], edgecolor=None)\n",
        "\n",
        "plot.set_xticklabels(['Negative', 'Positive'], rotation=0)\n",
        "plot.set_ylabel('Count')\n",
        "plot.set_xlabel('Sentiment')\n",
        "\n",
        "text, sentiment = list(df['text']), list(df['sentiment'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jQlnq9bDjOaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
        "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
        "\n",
        "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad',\n",
        "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
        "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed',\n",
        "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
        "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
        "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink',\n",
        "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
        "STOPWORDS = set(stopwordlist)\n",
        "# def cleaning_stopwords(text):\n",
        "#     return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "# def clean_text(s):\n",
        "#     s = re.sub(r'http\\S+', '', s)\n",
        "#     s = re.sub('(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)', ' ', s)\n",
        "#     s = re.sub(r'@\\S+', '', s)\n",
        "#     s = re.sub('&amp', ' ', s)\n",
        "#     return s\n",
        "\n",
        "# df['text'] = df['text'].apply(lambda text: cleaning_stopwords(text))\n",
        "# df['text'].head()\n",
        "# df['text'] = df['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "pjGMtYfLP2Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(text):\n",
        "    processed = []\n",
        "    wordLemm = WordNetLemmatizer()\n",
        "\n",
        "    url      = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "    user       = '@[^\\s]+'\n",
        "    alpha     = \"[^a-zA-Z0-9]\"\n",
        "    sequence = r\"(.)\\1\\1+\"\n",
        "    seqReplace = r\"\\1\\1\"\n",
        "\n",
        "    for t in text:\n",
        "        t = t.lower()\n",
        "\n",
        "        t = re.sub(url,' URL',t)\n",
        "        for emoji in emojis.keys():\n",
        "            t = t.replace(emoji, \"EMOJI\" + emojis[emoji])\n",
        "        t = re.sub(user,' USER', t)\n",
        "        t = re.sub(alpha, \" \", t)\n",
        "        t = re.sub(sequence, seqReplace, t)\n",
        "\n",
        "        tw = ''\n",
        "        for word in t.split():\n",
        "            if len(word)>1:\n",
        "                word = wordLemm.lemmatize(word)\n",
        "                tw += (word+' ')\n",
        "\n",
        "        processed.append(tw)\n",
        "\n",
        "    return processed"
      ],
      "metadata": {
        "id": "RLJEMvz5kzny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processedtext = preprocess_data(text)"
      ],
      "metadata": {
        "id": "Eg5xpOhmlbH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n",
        "                                                    test_size = 0.05, random_state = 0)\n"
      ],
      "metadata": {
        "id": "_K-tE8GYnZvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
        "vectoriser.fit(X_train)"
      ],
      "metadata": {
        "id": "EuHjXSJ1napf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = vectoriser.transform(X_train)\n",
        "X_test  = vectoriser.transform(X_test)"
      ],
      "metadata": {
        "id": "EFNatlUPojMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVC = LinearSVC()\n",
        "history1=SVC.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ue7tzjkk9w2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "y_pred = SVC.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plotting the simple confusion matrix\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cf_matrix, cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual labels')\n",
        "plt.xlabel('Predicted labels')\n",
        "tick_marks = np.arange(len(set(y_test))) # Assuming your labels are 0 and 1\n",
        "plt.xticks(tick_marks, ['Negative', 'Positive'], rotation=45)\n",
        "plt.yticks(tick_marks, ['Negative', 'Positive'])\n",
        "plt.colorbar()\n",
        "for i in range(cf_matrix.shape[0]):\n",
        "    for j in range(cf_matrix.shape[1]):\n",
        "        plt.text(j, i, format(cf_matrix[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cf_matrix[i, j] > cf_matrix.max() / 2. else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n"
      ],
      "metadata": {
        "id": "AnOvR8vw2F8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr, tpr, _ = roc_curve(y_test, SVC.decision_function(X_test))\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plotting the ROC curve\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aoDZX-6r-usA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n"
      ],
      "metadata": {
        "id": "aHWBf0kC_Amx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bernouli NB"
      ],
      "metadata": {
        "id": "BPtMDGx3_EDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BNB = BernoulliNB(alpha = 2)\n",
        "history2=BNB.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "2-qKAkOFpV_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = BNB.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plotting the simple confusion matrix\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cf_matrix, cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix for BNB')\n",
        "plt.ylabel('Actual labels')\n",
        "plt.xlabel('Predicted labels')\n",
        "tick_marks = np.arange(len(set(y_test))) # Assuming your labels are 0 and 1\n",
        "plt.xticks(tick_marks, ['Negative', 'Positive'], rotation=0)\n",
        "plt.yticks(tick_marks, ['Negative', 'Positive'])\n",
        "plt.colorbar()\n",
        "for i in range(cf_matrix.shape[0]):\n",
        "    for j in range(cf_matrix.shape[1]):\n",
        "        plt.text(j, i, format(cf_matrix[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cf_matrix[i, j] > cf_matrix.max() / 2. else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n"
      ],
      "metadata": {
        "id": "SqiVbYJrp5tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr, tpr, _ = roc_curve(y_test, SVC.decision_function(X_test))\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plotting the ROC curve\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for BNB')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QYwh3z21_hrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n"
      ],
      "metadata": {
        "id": "MXXvl8eb_nd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "hf37RS4PAMjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
        "history3= LR.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "dnVLJnTf_uz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = LR.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
        "# Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plotting the simple confusion matrix\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cf_matrix, cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix for LR')\n",
        "plt.ylabel('Actual labels')\n",
        "plt.xlabel('Predicted labels')\n",
        "tick_marks = np.arange(len(set(y_test))) # Assuming your labels are 0 and 1\n",
        "plt.xticks(tick_marks, ['Negative', 'Positive'], rotation=0)\n",
        "plt.yticks(tick_marks, ['Negative', 'Positive'])\n",
        "plt.colorbar()\n",
        "for i in range(cf_matrix.shape[0]):\n",
        "    for j in range(cf_matrix.shape[1]):\n",
        "        plt.text(j, i, format(cf_matrix[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cf_matrix[i, j] > cf_matrix.max() / 2. else \"black\")\n",
        "\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "ayPOwrGL_-8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ROC curve and ROC area for each class\n",
        "fpr, tpr, _ = roc_curve(y_test, SVC.decision_function(X_test))\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plotting the ROC curve\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for LR')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "25Wq9bEEAH5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing for CNN-LSTM Model"
      ],
      "metadata": {
        "id": "FruD2ZnZ_kY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train test split\n",
        "x=df.text\n",
        "y=df.sentiment\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y,\n",
        "    test_size=0.05, shuffle = True, random_state = 8)\n",
        "\n",
        "# Use the same function above for the validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
        "    test_size=0.1, random_state= 8)"
      ],
      "metadata": {
        "id": "FtGvPP71P9bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize\n",
        "max_features = 40000\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(X_train))\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "dpBvNuDhQKnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequencing for lstm\n",
        "max_words = 100\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words, padding = 'post')\n",
        "X_val = sequence.pad_sequences(X_val, maxlen=max_words, padding = 'post')\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words, padding = 'post')"
      ],
      "metadata": {
        "id": "NAM2iEWEQMPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating proper format and shape\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "y_test = np.array(y_test)\n",
        "print(X_train.shape,X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "Jf8e5tZLQNKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model"
      ],
      "metadata": {
        "id": "fBQN6hOsR2GL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(40000, 10, input_length=X_train.shape[1]))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1BL2tN06Rz8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=10, batch_size=256, verbose=1)"
      ],
      "metadata": {
        "id": "0oWNc3voR51W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.evaluate(X_test,y_test)\n",
        "result"
      ],
      "metadata": {
        "id": "5aD_m_NgR-jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib seaborn scikit-learn wordcloud"
      ],
      "metadata": {
        "id": "5l6mfwqN6y64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test contains true labels and y_pred contains predicted labels\n",
        "y_pred = model.predict(X_test)  # Adjust this line according to your model's prediction method\n",
        "y_pred_classes = np.argmax(y_pred, axis=1) if y_pred.shape[-1] > 1 else (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G5W9D0x562ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tqFmVXh963uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all text data with positive sentiment\n",
        "positive_text = \" \".join(text for text in df[df.sentiment == 0].text)\n",
        "\n",
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud(background_color=\"white\").generate(positive_text)\n",
        "\n",
        "# Display the generated image\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title('Word Cloud for Negative Sentiment')\n",
        "plt.show()\n",
        "\n",
        "# Repeat the process for negative sentiment by filtering `data[data.sentiment == \"negative\"].text`\n"
      ],
      "metadata": {
        "id": "W2VGWbid6613"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all text data with positive sentiment\n",
        "positive_text = \" \".join(text for text in df[df.sentiment == 1].text)\n",
        "\n",
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud(background_color=\"white\").generate(positive_text)\n",
        "\n",
        "# Display the generated image\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title('Word Cloud for Positive Sentiment')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2aMScb9pByQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming y_test contains true labels and y_pred_classes contains predicted labels\n",
        "# You already have y_pred_classes from your previous code\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test, y_pred_classes)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# Your existing code for confusion matrix visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test contains true labels and y_pred contains predicted labels\n",
        "# You have y_pred and y_pred_classes from earlier in your code\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6NAvl7HgrF4c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}